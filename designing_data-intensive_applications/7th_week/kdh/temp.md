> Hey I just met you  
The network’s laggy  
But here’s my data  
So store it maybe  


## fault and partial failure
싱글 컴퓨터 환경에서는 두가지 상황밖에 없다.  
- 정상 작동
- 작동 안함

작동안함의 경우 대개 소프트웨어의 문제이다.  
컴퓨터는 같은 입력이오면 같은 결과를 뱉을 뿐이기 때문이다.  
하지만 분산 컴퓨팅 환경에서는 다르다.  
> “In my limited experience I’ve dealt with long-lived network partitions in a single data center (DC), PDU [power distribution unit] failures, switch failures, accidental power cycles of whole racks, whole-DC backbone failures, whole-DC power failures, and a hypoglycemic driver smashing his Ford pickup truck into a DC’s HVAC [heating, ventilation, and air conditioning] system.  
> And I’m not even an ops guy.   
> -- Coda Hale”

정말 어떤 방식으로든 문제가 발생할 수 있고, 더 큰 문제는 이 것이 서비스의 완전한 에러가 아닌 부분적으로만 발생되게 된다는 것이다.  
때문에 문제를 인식하는 것조차 어려울 수 있다.  
때문에 정말 비관적으로 당연히 문제가 발생할 것이라 가정하고 이에 대응하는 소프트웨어 레벨에서의 설계를 해두는 것이 좋다.  

## unreliable Networks
이 책에서 다루는 분산시스템은 shared-nothing 시스템을 말한다.  
shared-nothing 시스템에서 노드끼리는 다른 노드의 메모리나 디스크를 직접 접근할 수 없다.  
네트워크를 통한 요청을 통해서만 다른 노드에 접근할 수 있다.  
분산 시스템을 구축하는 데에는 이런 shared-nothing 시스템만 있는 것이 아니지만 대부분의 경우 이를 사용한다.  
이유는 특별한 하드웨어가 필요하지 않고 상업용 컴퓨터를 그대로 사용하면 되기에 매우 싸기 때문이다.  
대신 이런 네트워크를 통해서만 노드끼리 소통하는 특징때문에 문제가 발생할 수 있는데, 네트워크는 요청이 언제 도착할지 또 완전히 도착할지를 보장해주지 않는다는 것이다.  
네트워크를 통해 요청을 보낼때 아래와 같은 문제가 발생할 수 있다.  
- 요청이 소실될 수 있다. 
- 요청이 늦게 도착할 수 있다.  
- 수신 노드가 고장날 수 있다. 
- 수신 노드가 요청을 처리하는데 오래 걸릴 수 있다.
- 수신 노드는 정상적으로 작동했지만 응답이 네트워크중에서 소실될 수 있다. 
- 수신 노드는 정상적으로 작동했지만 응답이 늦게 도착할 수 있다.

더욱 큰 문제는 위와같은 문제들이 발생했을때 아래 사진처럼 어떤것이 문제였는지 정확히 알 수가 없다.
![](why_distributed_system_is_hard.png)


### detect failure
많은 시스템이 네트워크 상에서 문제가 발생했는지 자동으로 감지하는 기능을 필요로 한다.     
- 로드 밸런서가 죽은 노드에는 요청을 보내지 않게끔 하기 
- 싱글리더 분산 환경에선 리더 노드가 죽었을 경우 팔로워 노드 중 하나가 승격되게 하는 것

이러한 네트워크 상에서의 문제의 불확실성은 노드가 고장났는지 아닌지조차 알기 어렵게 만들어버린다.  

요청은 정상적으로 도착했지만 노드에서 프로세스만 고장이 났을 경우  
프로세스가 요청을 처리하기도 전에 고장났다면 os가 다른 노드와 클라이언트에게 알려줄 수 있지만 만약 요청을 처리하던 중에 고장났을 경우 데이터가 얼마나 처리되었을지 알 방법이 없다.  

심지어 수신 노드가 고장이 났음을 알 수 있는 신호조차 받지 못하는 경우도 가정을 해야한다.  

### timeout delay
때문에 프로세스가 정상적으로 작동하는지 아닌지를 판단하기 위해 타임아웃을 대개 사용하는데 이 타임아웃의 기간도 잘 정해야한다.  
만약 타임아웃으로 판단하는 기간이 너무 길다면 사용자는 긴시간동안 기다리거나 에러 메세지를 봐야한다.  
하지만 너무 이르게 설정한다면?  
가장 단순한 문제로 일시적으로 노드가 느려져 요청을 시간내에 처리하지 못했을때 정상적인 노드가 죽은 노드로 판단되는 문제가 있고,  
최악의 경우 노드가 과부하 때문에 요청을 처리하는데에 오래 걸렸을때 이 노드가 죽은 노드로 판단되고 이 요청들이 다른 노드로 일임됐을때 연쇄적으로 모든 노드가 죽었다고 판단되는 경우가 있을 수도 있다.  

유토피아에서는 모든 네트워크 요청이 d 이내로 무조건 전해지고 요청이 r 이내로 무조건 처리된다 했을때,  
타임아웃을 2d + r로 한다면 정확히 이 시간보다 오래 걸린다면 네트워크나 노드에 문제가 있다고 판단할 수 있겠지만,  
현실 세계에선 네트워크 요청시간인 d는 물론이거니와 요청 처리시간인 r조차 보장해줄 수 없다.


### queue
또한 요청이란 이름의 물리적인 전기신호는 말그대로 물리적이기에 사실 그 양이 매우 크다면 처리할 수가 없다.  
때문에 큐를 사용하는데 아래와 같다. 
- 네트워크 상에서 한 수신지로 요청이 몰릴때 스위치 레벨에서 입력 신호를 큐에 대기 시켜놨다가 보낸다. 
  - 때문에 큐가 꽉찼다면 스위치는 정상적으로 작동함에도 데이터가 손실되어서 다시 보내줘야하는 경우가 생길 수 있다.
- 요청은 노드로 정상적으로 도착했지만 노드에서 모든 cpu core가 사용중이라 요청이 처리될 수 없는 경우 os가 요청을 큐에 대기시켜놨다가 cpu에 자리가 남으면 그때 보내준다. 
- vm을 사용할때 다른 vm이 cpu를 가져갈 경우 원래 vm은 일시정지되는데 이때는 요청을 처리할 수 없기에 vm에의해 요청들은 잠시 버퍼된다.
- TCP는 네트워크 혼잡을 피하기 위환 알고리즘을 사용하는데, 네트워크가 과부하되지 않기 위해 보내는 노드에서 자체적으로 요청을 보내는 양을 조절한다. 때문에 잠시 송신노드의 큐에 대기 시켜놨다가 보내기도 한다.  



## unreliable clocks
컴퓨터에서 시간을 측정하는 개념에는 두가지가 있다.  
- Time of Day
- Monotonic Clock

https://chatgpt.com/c/f2295131-0949-48f4-ae8c-2ed52c1ded21

time of day는 원자나 gps를 이용해 현재 시각을 정확히 재는 ntp서버로부터 값을 sync하기에 시간이동이 일어날 수 있다.  
때문에 값이 종종 변경될 수 있어, timeout delay가 경과됐는지 등의 작업을 하기엔 부적절하지만 정확한 시간을 나타내에는 적합하다.
반대로 monotonic clock은 특정 시점으로부 계속해 증가하는 값이기에, 값을 의도적으로 변경할 수 없다.  
때문에 어느 두 시점의 간격을 측정할때 유용하다.

컴퓨터에서 시간이란 결국에 내장한 oscillator의 주기에 의한 증가하는 값을 시간에 매핑하는 것이기에  
정확한 시간은 알 수가 없다.  
실제로 정확히 같은 시간에도 다른 두 컴퓨터에서는 현재 시각을 다르게 인식할 수 있다는 말이다.  
정확한 시간을 요구하는 시스템에서는 이 특징을 잘 이해하고 설계해야한다.  

